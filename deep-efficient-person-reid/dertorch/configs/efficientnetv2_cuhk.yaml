settings:
  #################   DATASET CONFIG   ###################

  dataset_names: "cuhk03" #['market1501', 'cuhk03']
  root_dir: "cuhk03_release" #['market1501','cuhk03_release']
  sampler: "softmax"
  seed: 2021

  #################   TRAINING CONFIG   ###################

  model_name: "efficientnet_v2" # efficientnet_v2, resnet18, resnet34, resnet50, resnet101, resnet152

  gpu_devices: "0" # supports multi-gpus
  batch_size: 32
  num_workers: 2
  num_instance: 2

  image_size: [256, 128] # should be square to prevent bugs [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]
  keep_ratio: False # whether to use resize padding

  mixup: True
  label_smooth: "on"
  margin: 0.3
  center_loss_weight: 0.0005
  loss_type: "triplet_center" # triplet, center, triplet_center
  if_with_center: "yes"

  # Transform
  prob: 0.5
  padding: 10
  re_prob: 0.5

  # Backbone details
  last_stride: 1
  neck: "bnneck"
  pretrain_path: "weights_efficientnetv2_cuhk/weights_efficientnetv2_cuhk/efficientnet_v2_model_60.pth" # ./weights_efficientnetv2_cuhk
  pretrain_choice: "self" # self, imagenet

  # If you want to use pretrained on ImageNet, set pretrained choice to "imagenet" and path to "efficientnetv2_imagenet.pt"

  # Trainer
  log_period: 50
  checkpoint_period: 25
  eval_period: 50
  output_dir: "./weights_efficientnetv2_cuhk_self"
  device: "cuda"
  num_epochs: 300

  # Test
  test_neck_feat: "after" # Which feature of BNNeck to be used for test, before or after BNNneck, options: 'before' or 'after'
  test_feat_norm: "yes" # Whether feature is nomalized before test, if yes, it is equivalent to cosine distance
  test_reranking: "no" # yes, no
  test_weight: "weights_efficientnetv2_cuhk/weights_efficientnetv2_cuhk/efficientnet_v2_model_60.pth"

  flip_feats: "off" # on, off
  log_dir: "loggers/runs/efficientnetv2_cuhk"
  query_dir: "market1501/query"
  dist_mat: "dist_mat.npy"
  pids: "pids.npy"
  camids: "camids.npy"
  img_path: "imgpath.npy"
  qfeats: "qfeats.pth" # query feats
  gfeats: "gfeats.pth" # gallery feats

  # learning rate policy
  lr_policy:
    name: "adam" #[adam|sgd]
    lr: 0.00035 #[adam: 1e-3 | sgd: 1e-2]
    momentum: 0.937
    weight_decay: 0.0005
    center_lr: 0.5
    weight_decay_bias: 0.
    bias_lr_factor: 2

  # scheduler
  steps: [30, 55]
  gamma: 0.1
  warmup_factor: 0.3
  warmup_iters: 500
  warmup_method: "linear"

  # whether to use mixed-precision
  mixed_precision: True
